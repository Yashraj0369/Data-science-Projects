{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":23404,"sourceType":"datasetVersion","datasetId":17860}],"dockerImageVersionId":30587,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # for data visualization\nimport seaborn as sns # for statistical data visualization\n%matplotlib inline\nimport warnings\n\nwarnings.filterwarnings('ignore')\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-07T16:26:51.937377Z","iopub.execute_input":"2023-12-07T16:26:51.937729Z","iopub.status.idle":"2023-12-07T16:26:53.933414Z","shell.execute_reply.started":"2023-12-07T16:26:51.937697Z","shell.execute_reply":"2023-12-07T16:26:53.931808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Machine+Learning+R/iris-machinelearning.png)","metadata":{}},{"cell_type":"markdown","source":"**About the IRIS dataset**","metadata":{}},{"cell_type":"markdown","source":"\n\n**Source**: The Iris dataset was introduced by the British biologist and statistician Ronald A. Fisher in 1936 as an example of discriminant analysis.\n\n**Data**: The dataset contains measurements of 150 iris flowers, with four features each: sepal length, sepal width, petal length, and petal width.\n\n**Classes**: The flowers belong to three different species: Setosa, Versicolor, and Virginica.\n\n**Balance**: Each class in the dataset has an equal number of 50 samples.\n\n**Usage**: The Iris dataset is commonly used as a beginner's dataset for practicing machine learning classification algorithms.\n\n**Exploratory Data Analysis (EDA)**: EDA is often performed on the Iris dataset to understand the distribution of features, relationships between variables, and class distributions.\n\n**Visualization**: Visualizations like pair plots, box plots, and scatter plots are frequently used to explore the dataset.\n\n**Machine Learning**: The dataset is popular for testing and demonstrating classification algorithms due to its simplicity and clear class separations.\n\n**Versatility**: Despite its simplicity, the Iris dataset remains relevant and is used in various machine learning tutorials, courses, and competitions.\n\n**Contribution**: Fisher's work on the Iris dataset laid the foundation for modern statistical techniques and machine learning, making it a historically significant dataset in the field.","metadata":{}},{"cell_type":"markdown","source":"**Import the data**","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/iris-flower-dataset/IRIS.csv')\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:26:53.935339Z","iopub.execute_input":"2023-12-07T16:26:53.936073Z","iopub.status.idle":"2023-12-07T16:26:53.966328Z","shell.execute_reply.started":"2023-12-07T16:26:53.93603Z","shell.execute_reply":"2023-12-07T16:26:53.965182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Check head of data**","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:26:53.969207Z","iopub.execute_input":"2023-12-07T16:26:53.969866Z","iopub.status.idle":"2023-12-07T16:26:54.019397Z","shell.execute_reply.started":"2023-12-07T16:26:53.969825Z","shell.execute_reply":"2023-12-07T16:26:54.018168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**check column names**","metadata":{}},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:26:54.021073Z","iopub.execute_input":"2023-12-07T16:26:54.022365Z","iopub.status.idle":"2023-12-07T16:26:54.033222Z","shell.execute_reply.started":"2023-12-07T16:26:54.02232Z","shell.execute_reply":"2023-12-07T16:26:54.031835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**check target column values**","metadata":{}},{"cell_type":"code","source":"# check distribution of target_class column\ndf['species'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:26:54.034839Z","iopub.execute_input":"2023-12-07T16:26:54.035826Z","iopub.status.idle":"2023-12-07T16:26:54.05441Z","shell.execute_reply.started":"2023-12-07T16:26:54.03577Z","shell.execute_reply":"2023-12-07T16:26:54.053204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Summary statistics**","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:26:54.05607Z","iopub.execute_input":"2023-12-07T16:26:54.056775Z","iopub.status.idle":"2023-12-07T16:26:54.080903Z","shell.execute_reply.started":"2023-12-07T16:26:54.056737Z","shell.execute_reply":"2023-12-07T16:26:54.079899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Missing values check**","metadata":{}},{"cell_type":"markdown","source":"![](https://images.squarespace-cdn.com/content/v1/5e714fa1f7d532536a90c6e0/ec3a30a5-f850-497e-9cb0-774f29cb5553/missing+values.png)","metadata":{}},{"cell_type":"code","source":"# check for missing values in variables\n\ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:26:54.082527Z","iopub.execute_input":"2023-12-07T16:26:54.083216Z","iopub.status.idle":"2023-12-07T16:26:54.093716Z","shell.execute_reply.started":"2023-12-07T16:26:54.083177Z","shell.execute_reply":"2023-12-07T16:26:54.092507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"round(df.describe(),2)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:26:54.099232Z","iopub.execute_input":"2023-12-07T16:26:54.09998Z","iopub.status.idle":"2023-12-07T16:26:54.133159Z","shell.execute_reply.started":"2023-12-07T16:26:54.099935Z","shell.execute_reply":"2023-12-07T16:26:54.132384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****EDA****","metadata":{}},{"cell_type":"markdown","source":"![](https://editor.analyticsvidhya.com/uploads/61798ti2.png)","metadata":{}},{"cell_type":"code","source":"# Pairplot to visualize relationships between features\nsns.pairplot(df, hue='species', markers=['o', 's', 'D'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:26:54.134581Z","iopub.execute_input":"2023-12-07T16:26:54.13523Z","iopub.status.idle":"2023-12-07T16:27:01.225215Z","shell.execute_reply.started":"2023-12-07T16:26:54.1352Z","shell.execute_reply":"2023-12-07T16:27:01.223781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Violin plots for each feature\nplt.figure(figsize=(14, 8))\nfor i, column in enumerate(df.columns[:-1]):\n    plt.subplot(2, 2, i + 1)\n    sns.violinplot(x='species', y=column, data=df, inner='quartile')\n    plt.title(f'{column} distribution by species')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:27:01.226529Z","iopub.execute_input":"2023-12-07T16:27:01.226914Z","iopub.status.idle":"2023-12-07T16:27:02.420334Z","shell.execute_reply.started":"2023-12-07T16:27:01.226883Z","shell.execute_reply":"2023-12-07T16:27:02.419168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Swarm plots for each feature\nplt.figure(figsize=(12, 8))\nfor i, column in enumerate(df.columns[:-1]):\n    plt.subplot(2, 2, i + 1)\n    sns.swarmplot(x='species', y=column, data=df)\n    plt.title(f'{column} distribution by class')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:27:02.421607Z","iopub.execute_input":"2023-12-07T16:27:02.421926Z","iopub.status.idle":"2023-12-07T16:27:05.693244Z","shell.execute_reply.started":"2023-12-07T16:27:02.421899Z","shell.execute_reply":"2023-12-07T16:27:05.692129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# draw boxplots to visualize outliers\n\nplt.figure(figsize=(15,15))\n\n\nplt.subplot(4, 2, 1)\nfig = df.boxplot(column='sepal_length')\nfig.set_title('')\nfig.set_ylabel('sepal_length')\n\n\nplt.subplot(4, 2, 2)\nfig = df.boxplot(column='sepal_width')\nfig.set_title('')\nfig.set_ylabel('sepal_width')\n\n\nplt.subplot(4, 2, 3)\nfig = df.boxplot(column='petal_length')\nfig.set_title('')\nfig.set_ylabel('petal_length')\n\n\nplt.subplot(4, 2, 4)\nfig = df.boxplot(column='petal_width')\nfig.set_title('')\nfig.set_ylabel('petal_width')","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:27:05.694936Z","iopub.execute_input":"2023-12-07T16:27:05.695712Z","iopub.status.idle":"2023-12-07T16:27:06.486296Z","shell.execute_reply.started":"2023-12-07T16:27:05.695666Z","shell.execute_reply":"2023-12-07T16:27:06.485199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot histogram to check distribution\n\n\nplt.figure(figsize=(24,20))\n\n\nplt.subplot(4, 2, 1)\nfig = df['sepal_length'].hist(bins=10)\nfig.set_xlabel('IP Mean')\n\n\n\nplt.subplot(4, 2, 2)\nfig = df['sepal_width'].hist(bins=10)\nfig.set_xlabel('sepal_width')\n\n\n\nplt.subplot(4, 2, 3)\nfig = df['petal_length'].hist(bins=10)\nfig.set_xlabel('petal_length')\n\n\n\nplt.subplot(4, 2, 4)\nfig = df['petal_width'].hist(bins=10)\nfig.set_xlabel('petal_width')\n","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:27:06.488074Z","iopub.execute_input":"2023-12-07T16:27:06.488932Z","iopub.status.idle":"2023-12-07T16:27:07.611127Z","shell.execute_reply.started":"2023-12-07T16:27:06.488891Z","shell.execute_reply":"2023-12-07T16:27:07.610058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Get X and Y**","metadata":{}},{"cell_type":"code","source":"X = df.drop(['species'], axis=1)\n\ny = df['species']","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:27:07.612945Z","iopub.execute_input":"2023-12-07T16:27:07.613701Z","iopub.status.idle":"2023-12-07T16:27:07.621701Z","shell.execute_reply.started":"2023-12-07T16:27:07.61366Z","shell.execute_reply":"2023-12-07T16:27:07.6202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](https://miro.medium.com/v2/resize:fit:580/1*OECM6SWmlhVzebmSuvMtBg.png)","metadata":{}},{"cell_type":"code","source":"# split X and y into training and testing sets\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 10)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:27:07.623401Z","iopub.execute_input":"2023-12-07T16:27:07.623825Z","iopub.status.idle":"2023-12-07T16:27:07.840157Z","shell.execute_reply.started":"2023-12-07T16:27:07.623785Z","shell.execute_reply":"2023-12-07T16:27:07.839158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape, X_test.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:27:07.841587Z","iopub.execute_input":"2023-12-07T16:27:07.842775Z","iopub.status.idle":"2023-12-07T16:27:07.851554Z","shell.execute_reply.started":"2023-12-07T16:27:07.842732Z","shell.execute_reply":"2023-12-07T16:27:07.850148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Feature Scaling**","metadata":{}},{"cell_type":"markdown","source":"use of Standard Scaler and feature scaling for SVM modeling on the Iris dataset:\n\nMotivation: 💡 Standard Scaler is a key player in ML pipelines, harmonizing dataset features. For SVM models, particularly those finicky about feature scales, scaling is a game-changer.\n\nStandard Scaler: 📏 The Standard Scaler dances with data, aligning it to the mean and grooving it to a standard deviation of 1. This ensures that features share the same vibes, preventing some from stealing the spotlight.\n\nEffect on SVM: 🤖 Support Vector Machines (SVMs) are choosy about feature scales. In SVM, the decision boundary sways with the feature scale, and having features on a similar scale unleashes the algorithm's full potential.\n\nPreprocessing Step: 🚀 Scaling takes the stage as a crucial preprocessing step, especially in the SVM arena, where the algorithm's magic relies on the distance between data points. 📊","metadata":{}},{"cell_type":"code","source":"cols = X_train.columns","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:27:07.853332Z","iopub.execute_input":"2023-12-07T16:27:07.853774Z","iopub.status.idle":"2023-12-07T16:27:07.861941Z","shell.execute_reply.started":"2023-12-07T16:27:07.853735Z","shell.execute_reply":"2023-12-07T16:27:07.860662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\n\nX_test = scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:27:07.863232Z","iopub.execute_input":"2023-12-07T16:27:07.863675Z","iopub.status.idle":"2023-12-07T16:27:07.881911Z","shell.execute_reply.started":"2023-12-07T16:27:07.863633Z","shell.execute_reply":"2023-12-07T16:27:07.880525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = pd.DataFrame(X_train, columns=[cols])","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:27:07.883434Z","iopub.execute_input":"2023-12-07T16:27:07.883855Z","iopub.status.idle":"2023-12-07T16:27:07.894925Z","shell.execute_reply.started":"2023-12-07T16:27:07.883818Z","shell.execute_reply":"2023-12-07T16:27:07.893382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = pd.DataFrame(X_test, columns=[cols])","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:27:07.898263Z","iopub.execute_input":"2023-12-07T16:27:07.898959Z","iopub.status.idle":"2023-12-07T16:27:07.907429Z","shell.execute_reply.started":"2023-12-07T16:27:07.898916Z","shell.execute_reply":"2023-12-07T16:27:07.906068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:27:07.911442Z","iopub.execute_input":"2023-12-07T16:27:07.912273Z","iopub.status.idle":"2023-12-07T16:27:07.932688Z","shell.execute_reply.started":"2023-12-07T16:27:07.912228Z","shell.execute_reply":"2023-12-07T16:27:07.931515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Default hyperparameter: C=1.0, kernel=rbf and gamma=auto","metadata":{}},{"cell_type":"markdown","source":"![](https://i.ytimg.com/vi/ny1iZ5A8ilA/maxresdefault.jpg)","metadata":{}},{"cell_type":"code","source":"# import SVC classifier\nfrom sklearn.svm import SVC\n\n# import metrics to compute accuracy\nfrom sklearn.metrics import accuracy_score\n\n# instantiate classifier with default hyperparameters\nsvc=SVC() \n\n# fit classifier to training set\nsvc.fit(X_train,y_train)\n\n# make predictions on test set\ny_pred=svc.predict(X_test)\n\n# compute and print accuracy score\nprint('Model accuracy score with default hyperparameters: {0:0.2f}'. format(accuracy_score(y_test, y_pred)))","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:27:07.936082Z","iopub.execute_input":"2023-12-07T16:27:07.937012Z","iopub.status.idle":"2023-12-07T16:27:08.072744Z","shell.execute_reply.started":"2023-12-07T16:27:07.936979Z","shell.execute_reply":"2023-12-07T16:27:08.071381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**compare the train-set and test-set accuracy to check for overfitting**","metadata":{}},{"cell_type":"code","source":"y_pred_train = svc.predict(X_train)\n\ny_pred_train","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:27:08.079764Z","iopub.execute_input":"2023-12-07T16:27:08.080231Z","iopub.status.idle":"2023-12-07T16:27:08.092372Z","shell.execute_reply.started":"2023-12-07T16:27:08.080192Z","shell.execute_reply":"2023-12-07T16:27:08.091166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:27:08.093835Z","iopub.execute_input":"2023-12-07T16:27:08.094763Z","iopub.status.idle":"2023-12-07T16:27:08.104753Z","shell.execute_reply.started":"2023-12-07T16:27:08.094732Z","shell.execute_reply":"2023-12-07T16:27:08.103618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nOverfitting occurs when a model performs very well on the training data but poorly on new, unseen data.\nThis can happen when the model is too complex and captures noise or specific patterns in the training data that don't generalize well.\nUnderfitting:\nUnderfitting occurs when a model performs poorly on both the training and test data.\nIt suggests that the model is too simple to capture the underlying patterns in the data.","metadata":{}},{"cell_type":"markdown","source":"Ideally, you want the test-set accuracy to be close to the train-set accuracy. If the model generalizes well, both accuracies should be similar.\nA significant drop in test-set accuracy compared to train-set accuracy may indicate overfitting.The scores here are similar and satisfactory","metadata":{}},{"cell_type":"markdown","source":"**Compare model accuracy with null accuracy**\n\nSo, the model accuracy is 1.00. But, we cannot say that our model is perfect based on the above accuracy. We must compare it with the null accuracy. Null accuracy is the accuracy that could be achieved by always predicting the most frequent class. In our dataset all the 3 classes have equal frequency hence not needed.","metadata":{}},{"cell_type":"markdown","source":"Using Confusion Matrix","metadata":{}},{"cell_type":"code","source":"# Print the Confusion Matrix and slice it into four pieces\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, y_pred)\n\nprint('Confusion matrix\\n\\n', cm)\n\nprint('\\nTrue Positives(TP) = ', cm[0,0])\n\nprint('\\nTrue Negatives(TN) = ', cm[1,1])\n\nprint('\\nFalse Positives(FP) = ', cm[0,1])\n\nprint('\\nFalse Negatives(FN) = ', cm[1,0])","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:27:08.106504Z","iopub.execute_input":"2023-12-07T16:27:08.107269Z","iopub.status.idle":"2023-12-07T16:27:08.120663Z","shell.execute_reply.started":"2023-12-07T16:27:08.107227Z","shell.execute_reply":"2023-12-07T16:27:08.118967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize confusion matrix with seaborn heatmap\n\nsns.heatmap(cm, annot=True, fmt='d', cmap='YlGnBu')","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:27:08.122128Z","iopub.execute_input":"2023-12-07T16:27:08.122742Z","iopub.status.idle":"2023-12-07T16:27:08.656874Z","shell.execute_reply.started":"2023-12-07T16:27:08.122707Z","shell.execute_reply":"2023-12-07T16:27:08.656062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Classification Report\n\nClassification report is another way to evaluate the classification model performance. It displays the precision, recall, f1 and support scores for the model","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:27:08.658206Z","iopub.execute_input":"2023-12-07T16:27:08.659285Z","iopub.status.idle":"2023-12-07T16:27:08.672925Z","shell.execute_reply.started":"2023-12-07T16:27:08.659253Z","shell.execute_reply":"2023-12-07T16:27:08.67217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TP = cm[0,0]\nTN = cm[1,1]\nFP = cm[0,1]\nFN = cm[1,0]\n# print classification accuracy\n\nclassification_accuracy = (TP + TN) / float(TP + TN + FP + FN)\n\nprint('Classification accuracy : {0:0.3f}'.format(classification_accuracy))","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:27:08.674285Z","iopub.execute_input":"2023-12-07T16:27:08.674857Z","iopub.status.idle":"2023-12-07T16:27:08.681283Z","shell.execute_reply.started":"2023-12-07T16:27:08.674825Z","shell.execute_reply":"2023-12-07T16:27:08.680241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print classification error\n\nclassification_error = (FP + FN) / float(TP + TN + FP + FN)\n\nprint('Classification error : {0:0.4f}'.format(classification_error))","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:27:08.682599Z","iopub.execute_input":"2023-12-07T16:27:08.682926Z","iopub.status.idle":"2023-12-07T16:27:08.693345Z","shell.execute_reply.started":"2023-12-07T16:27:08.682899Z","shell.execute_reply":"2023-12-07T16:27:08.692179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print precision score\n\nprecision = TP / float(TP + FP)\n\n\nprint('Precision : {0:0.4f}'.format(precision))","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:27:08.694892Z","iopub.execute_input":"2023-12-07T16:27:08.695289Z","iopub.status.idle":"2023-12-07T16:27:08.704554Z","shell.execute_reply.started":"2023-12-07T16:27:08.695259Z","shell.execute_reply":"2023-12-07T16:27:08.703355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recall = TP / float(TP + FN)\n\nprint('Recall or Sensitivity : {0:0.4f}'.format(recall))","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:27:08.706173Z","iopub.execute_input":"2023-12-07T16:27:08.706521Z","iopub.status.idle":"2023-12-07T16:27:08.715216Z","shell.execute_reply.started":"2023-12-07T16:27:08.706494Z","shell.execute_reply":"2023-12-07T16:27:08.713991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n\nkfold=KFold(n_splits=5, shuffle=True, random_state=0)\n\n\nlinear_svc=SVC()\n\n\nlinear_scores = cross_val_score(linear_svc, X, y, cv=kfold)\n# print cross-validation scores with linear kernel\n\nprint('cross-validation scores with linear kernel:\\n\\n{}'.format(linear_scores))\nprint('mean cross-validation scores with linear kernel:\\n\\n{}'.format(linear_scores.mean()))","metadata":{"execution":{"iopub.status.busy":"2023-12-07T16:27:08.71668Z","iopub.execute_input":"2023-12-07T16:27:08.717102Z","iopub.status.idle":"2023-12-07T16:27:08.762729Z","shell.execute_reply.started":"2023-12-07T16:27:08.71706Z","shell.execute_reply":"2023-12-07T16:27:08.761837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Conclusion*\n\nOur standard model of SVM with default parameters of SVC(C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=1e-3, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)\ngave us 100 percent accuracy. We also checked for overfitting which is not present.We also applied KFOLD Cross validation technique which gave us a mean score of 0.94","metadata":{}},{"cell_type":"markdown","source":"![](https://t3.ftcdn.net/jpg/02/91/52/22/360_F_291522205_XkrmS421FjSGTMRdTrqFZPxDY19VxpmL.jpg)","metadata":{}}]}